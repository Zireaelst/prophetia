"""
PROPHETIA Algorithm Test Suite - Week 4
========================================

Tests all three ML algorithm implementations:
1. Linear Regression (algorithm_id = 1)
2. Logistic Regression (algorithm_id = 2)  
3. Decision Tree (algorithm_id = 3)

This suite validates:
- Correct computation for each algorithm type
- Sigmoid approximation accuracy
- Decision tree path traversal
- Algorithm performance comparison
- Edge cases and boundary conditions
"""

import math
from typing import Dict, List, Tuple

# Fixed-point scale (same as Leo contracts)
SCALE = 1_000_000


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# HELPER FUNCTIONS - MIRROR LEO IMPLEMENTATION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def fixed_mul(a: int, b: int) -> int:
    """Multiply two fixed-point numbers"""
    return (a * b) // SCALE


def fixed_add(a: int, b: int) -> int:
    """Add two fixed-point numbers"""
    return a + b


def fixed_sub(a: int, b: int) -> int:
    """Subtract two fixed-point numbers (returns absolute value)"""
    return abs(a - b)


def weighted_sum(weights: List[int], features: List[int]) -> int:
    """Compute weighted sum of features"""
    result = 0
    for w, f in zip(weights, features):
        result += fixed_mul(w, f)
    return result


def sigmoid_approx(x: int) -> int:
    """
    Approximate sigmoid function (Leo implementation)
    
    sigmoid(x) â‰ˆ 0.5 + x/12 for |x| < 6
    sigmoid(x) â‰ˆ 1.0 for x >= 6
    sigmoid(x) â‰ˆ 0.0 for x <= -6
    """
    if x >= 6_000_000:
        return 1_000_000
    
    # Linear approximation
    result = 500_000 + (x // 12)
    
    # Clamp to [0, 1]
    if result > 1_000_000:
        result = 1_000_000
    if result < 0:
        result = 0
    
    return result


def abs_diff(a: int, b: int) -> int:
    """Absolute difference between two values"""
    return abs(a - b)


def clamp(value: int, min_val: int, max_val: int) -> int:
    """Clamp value to range [min_val, max_val]"""
    if value < min_val:
        return min_val
    elif value > max_val:
        return max_val
    else:
        return value


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ALGORITHM IMPLEMENTATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def divine_future_linear(data: Dict, model: Dict) -> Dict:
    """
    Linear regression inference (algorithm_id = 1)
    
    Formula:
        score = Î£(weights[i] Ã— features[i]) + bias
        direction = score >= threshold
        confidence = |score - threshold| / threshold
    """
    # Feature engineering
    features = [
        data['payload'],
        data['quality_score'],
        1_000_000,  # 1.0
        500_000     # 0.5
    ]
    
    # Weighted sum
    score = weighted_sum(model['weights'], features)
    score = fixed_add(score, model['bias'])
    
    # Activation
    direction = score >= model['threshold']
    
    # Confidence calculation
    if score >= model['threshold']:
        confidence = (score - model['threshold']) * SCALE // model['threshold']
    else:
        confidence = (model['threshold'] - score) * SCALE // model['threshold']
    
    # Clamp confidence
    confidence = clamp(confidence, 0, 1_000_000)
    
    return {
        'direction': direction,
        'confidence': confidence,
        'score': score,
        'algorithm': 'linear'
    }


def divine_future_logistic(data: Dict, model: Dict) -> Dict:
    """
    Logistic regression inference (algorithm_id = 2)
    
    Formula:
        score = Î£(weights[i] Ã— features[i]) + bias
        probability = sigmoid(score)
        direction = probability >= 0.5
        confidence = 2 Ã— |probability - 0.5|
    """
    # Feature engineering
    features = [
        data['payload'],
        data['quality_score'],
        1_000_000,  # 1.0
        500_000     # 0.5
    ]
    
    # Weighted sum
    score = weighted_sum(model['weights'], features)
    score = fixed_add(score, model['bias'])
    
    # Sigmoid activation
    probability = sigmoid_approx(score)
    
    # Decision
    direction = probability >= 500_000
    
    # Confidence calculation
    confidence = abs_diff(probability, 500_000)
    confidence = confidence * 2  # Scale to [0, 1]
    confidence = clamp(confidence, 0, 1_000_000)
    
    return {
        'direction': direction,
        'confidence': confidence,
        'probability': probability,
        'score': score,
        'algorithm': 'logistic'
    }


def divine_future_tree(data: Dict, model: Dict) -> Dict:
    """
    Decision tree inference (algorithm_id = 3)
    
    Tree structure:
        if payload > weights[0]:
            if quality_score > weights[1]:
                UP (confidence 0.8)
            else:
                DOWN (confidence 0.6)
        else:
            if quality_score > weights[2]:
                UP (confidence 0.7)
            else:
                DOWN (confidence 0.9)
    """
    direction = False
    confidence = 500_000
    path = ""
    
    # Level 1: Check payload
    if data['payload'] > model['weights'][0]:
        # Left branch
        if data['quality_score'] > model['weights'][1]:
            # Leaf 1
            direction = True
            confidence = 800_000
            path = "LEFT->LEFT (High payload, High quality)"
        else:
            # Leaf 2
            direction = False
            confidence = 600_000
            path = "LEFT->RIGHT (High payload, Low quality)"
    else:
        # Right branch
        if data['quality_score'] > model['weights'][2]:
            # Leaf 3
            direction = True
            confidence = 700_000
            path = "RIGHT->LEFT (Low payload, High quality)"
        else:
            # Leaf 4
            direction = False
            confidence = 900_000
            path = "RIGHT->RIGHT (Low payload, Low quality)"
    
    # Bias adjustment
    confidence = fixed_add(confidence, model['bias'])
    confidence = clamp(confidence, 0, 1_000_000)
    
    return {
        'direction': direction,
        'confidence': confidence,
        'path': path,
        'algorithm': 'tree'
    }


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST RESULTS TRACKER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class TestResults:
    def __init__(self):
        self.total = 0
        self.passed = 0
        self.failed = 0
        self.tests = []
    
    def add(self, name: str, passed: bool, message: str = ""):
        self.total += 1
        if passed:
            self.passed += 1
            status = "âœ… PASS"
        else:
            self.failed += 1
            status = "âŒ FAIL"
        
        self.tests.append((name, status, message))
        print(f"{status}: {name}")
        if message:
            print(f"  â†’ {message}")
    
    def summary(self):
        print("\n" + "=" * 70)
        print("TEST SUMMARY")
        print("=" * 70)
        print(f"Total Tests: {self.total}")
        print(f"Passed: {self.passed}")
        print(f"Failed: {self.failed}")
        print(f"Pass Rate: {(self.passed/self.total*100):.1f}%")
        
        if self.failed > 0:
            print("\nâŒ FAILED TESTS:")
            for name, status, msg in self.tests:
                if "FAIL" in status:
                    print(f"  - {name}: {msg}")
        else:
            print("\nâœ… ALL TESTS PASSED!")


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# TEST SUITE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def test_linear_regression(results: TestResults):
    """Test linear regression algorithm (Week 3 validation)"""
    
    print("\n" + "â”€" * 70)
    print("TEST CATEGORY: Linear Regression")
    print("â”€" * 70)
    
    # Test 1: Simple upward prediction
    data = {
        'payload': 1_500_000,  # $1.50
        'quality_score': 900_000,  # 0.9
        'category': 1
    }
    
    model = {
        'weights': [600_000, 100_000, 200_000, 100_000],
        'bias': 100_000,
        'threshold': 1_000_000,
        'algorithm_id': 1
    }
    
    result = divine_future_linear(data, model)
    
    # Expected: score = 0.6*1.5 + 0.1*0.9 + 0.2*1.0 + 0.1*0.5 + 0.1 = 1.34
    results.add(
        "Linear: Simple upward prediction",
        result['direction'] == True and result['score'] == 1_340_000,
        f"Direction={result['direction']}, Score={result['score']/SCALE:.2f}"
    )
    
    # Test 2: Downward prediction
    data2 = {
        'payload': 500_000,  # $0.50
        'quality_score': 600_000,  # 0.6
        'category': 1
    }
    
    result2 = divine_future_linear(data2, model)
    
    results.add(
        "Linear: Downward prediction",
        result2['direction'] == False,
        f"Direction={result2['direction']}, Score={result2['score']/SCALE:.2f}"
    )


def test_logistic_regression(results: TestResults):
    """Test logistic regression with sigmoid activation"""
    
    print("\n" + "â”€" * 70)
    print("TEST CATEGORY: Logistic Regression")
    print("â”€" * 70)
    
    # Test 1: High probability upward
    data = {
        'payload': 2_000_000,  # $2.00
        'quality_score': 850_000,  # 0.85
        'category': 1
    }
    
    model = {
        'weights': [400_000, 400_000, 150_000, 150_000],
        'bias': 50_000,
        'threshold': 500_000,  # Not used for logistic
        'algorithm_id': 2
    }
    
    result = divine_future_logistic(data, model)
    
    # Expected: score = 0.4*2 + 0.4*0.85 + 0.15*1 + 0.15*0.5 + 0.05 = 1.415
    # probability = sigmoid(1.415) â‰ˆ 0.8 (from approximation)
    
    results.add(
        "Logistic: High probability UP",
        result['direction'] == True and result['probability'] > 500_000,
        f"Direction={result['direction']}, Probability={result['probability']/SCALE:.3f}, Confidence={result['confidence']/SCALE:.2f}"
    )
    
    # Test 2: Low probability (should be DOWN)
    data2 = {
        'payload': 500_000,  # $0.50
        'quality_score': 400_000,  # 0.4
        'category': 1
    }
    
    result2 = divine_future_logistic(data2, model)
    
    results.add(
        "Logistic: Low score prediction",
        result2['probability'] >= 0,  # Valid probability
        f"Direction={result2['direction']}, Probability={result2['probability']/SCALE:.3f}"
    )
    
    # Test 3: Exact decision boundary
    # Score that gives probability â‰ˆ 0.5
    data3 = {
        'payload': 0,
        'quality_score': 0,
        'category': 1
    }
    
    model3 = {
        'weights': [0, 0, 500_000, 0],  # Only constant feature matters
        'bias': 0,
        'threshold': 500_000,
        'algorithm_id': 2
    }
    
    result3 = divine_future_logistic(data3, model3)
    
    # Expected: score = 0.5, probability = sigmoid(0.5) â‰ˆ 0.54
    results.add(
        "Logistic: Near decision boundary",
        abs(result3['probability'] - 500_000) < 200_000,  # Within 20% of 0.5
        f"Probability={result3['probability']/SCALE:.3f} (should be near 0.5)"
    )


def test_decision_tree_all_paths(results: TestResults):
    """Test decision tree algorithm - all 4 leaf paths"""
    
    print("\n" + "â”€" * 70)
    print("TEST CATEGORY: Decision Tree (All Paths)")
    print("â”€" * 70)
    
    model = {
        'weights': [1_500_000, 900_000, 700_000, 0],  # Thresholds: 1.5, 0.9, 0.7
        'bias': 100_000,  # +0.1 confidence boost
        'threshold': 1_000_000,
        'algorithm_id': 3
    }
    
    # Path 1: High payload, High quality â†’ UP (0.8 + 0.1 = 0.9)
    data1 = {
        'payload': 1_800_000,  # 1.8 > 1.5
        'quality_score': 950_000,  # 0.95 > 0.9
        'category': 1
    }
    
    result1 = divine_future_tree(data1, model)
    
    results.add(
        "Tree Path 1: HIGH payload, HIGH quality",
        result1['direction'] == True and result1['confidence'] == 900_000,
        f"{result1['path']} â†’ {result1['direction']}, Confidence={result1['confidence']/SCALE:.2f}"
    )
    
    # Path 2: High payload, Low quality â†’ DOWN (0.6 + 0.1 = 0.7)
    data2 = {
        'payload': 1_800_000,  # 1.8 > 1.5
        'quality_score': 800_000,  # 0.8 < 0.9
        'category': 1
    }
    
    result2 = divine_future_tree(data2, model)
    
    results.add(
        "Tree Path 2: HIGH payload, LOW quality",
        result2['direction'] == False and result2['confidence'] == 700_000,
        f"{result2['path']} â†’ {result2['direction']}, Confidence={result2['confidence']/SCALE:.2f}"
    )
    
    # Path 3: Low payload, High quality â†’ UP (0.7 + 0.1 = 0.8)
    data3 = {
        'payload': 1_200_000,  # 1.2 < 1.5
        'quality_score': 800_000,  # 0.8 > 0.7
        'category': 1
    }
    
    result3 = divine_future_tree(data3, model)
    
    results.add(
        "Tree Path 3: LOW payload, HIGH quality",
        result3['direction'] == True and result3['confidence'] == 800_000,
        f"{result3['path']} â†’ {result3['direction']}, Confidence={result3['confidence']/SCALE:.2f}"
    )
    
    # Path 4: Low payload, Low quality â†’ DOWN (0.9 + 0.1 = 1.0)
    data4 = {
        'payload': 1_200_000,  # 1.2 < 1.5
        'quality_score': 600_000,  # 0.6 < 0.7
        'category': 1
    }
    
    result4 = divine_future_tree(data4, model)
    
    results.add(
        "Tree Path 4: LOW payload, LOW quality",
        result4['direction'] == False and result4['confidence'] == 1_000_000,
        f"{result4['path']} â†’ {result4['direction']}, Confidence={result4['confidence']/SCALE:.2f}"
    )


def test_algorithm_comparison(results: TestResults):
    """Compare all three algorithms on same data"""
    
    print("\n" + "â”€" * 70)
    print("TEST CATEGORY: Algorithm Comparison")
    print("â”€" * 70)
    
    # Same data for all algorithms
    data = {
        'payload': 1_600_000,  # $1.60
        'quality_score': 880_000,  # 0.88
        'category': 1
    }
    
    # Linear model
    linear_model = {
        'weights': [500_000, 300_000, 200_000, 100_000],
        'bias': 100_000,
        'threshold': 1_000_000,
        'algorithm_id': 1
    }
    
    # Logistic model
    logistic_model = {
        'weights': [400_000, 400_000, 150_000, 150_000],
        'bias': 50_000,
        'threshold': 500_000,
        'algorithm_id': 2
    }
    
    # Tree model
    tree_model = {
        'weights': [1_500_000, 900_000, 700_000, 0],
        'bias': 100_000,
        'threshold': 1_000_000,
        'algorithm_id': 3
    }
    
    # Run all three
    result_linear = divine_future_linear(data, linear_model)
    result_logistic = divine_future_logistic(data, logistic_model)
    result_tree = divine_future_tree(data, tree_model)
    
    print(f"\nğŸ“Š COMPARISON RESULTS:")
    print(f"  Linear:   Direction={result_linear['direction']}, Confidence={result_linear['confidence']/SCALE:.2f}")
    print(f"  Logistic: Direction={result_logistic['direction']}, Confidence={result_logistic['confidence']/SCALE:.2f}, Prob={result_logistic['probability']/SCALE:.3f}")
    print(f"  Tree:     Direction={result_tree['direction']}, Confidence={result_tree['confidence']/SCALE:.2f}, Path={result_tree['path']}")
    
    # All algorithms should produce valid outputs
    results.add(
        "Comparison: All algorithms produce valid outputs",
        (result_linear['confidence'] >= 0 and 
         result_logistic['confidence'] >= 0 and 
         result_tree['confidence'] >= 0),
        "All three algorithms completed successfully"
    )
    
    # Algorithms can disagree (this is expected!)
    results.add(
        "Comparison: Algorithms can produce different results",
        True,  # Always pass - just demonstrating variety
        f"Linear={result_linear['direction']}, Logistic={result_logistic['direction']}, Tree={result_tree['direction']}"
    )


def test_sigmoid_approximation(results: TestResults):
    """Test sigmoid approximation accuracy"""
    
    print("\n" + "â”€" * 70)
    print("TEST CATEGORY: Sigmoid Approximation")
    print("â”€" * 70)
    
    # Test cases with expected ranges
    test_cases = [
        (0, 500_000, "sigmoid(0) should be 0.5"),
        (6_000_000, 1_000_000, "sigmoid(6) should be 1.0"),
        (3_000_000, 700_000, "sigmoid(3) should be ~0.75"),  # Approximation
        (1_000_000, 583_333, "sigmoid(1) should be ~0.58"),  # Approximation
    ]
    
    for x, expected, desc in test_cases:
        result = sigmoid_approx(x)
        error = abs(result - expected) / SCALE
        
        # Allow 10% error for approximation
        passed = abs(result - expected) < 100_000
        
        results.add(
            f"Sigmoid: {desc}",
            passed,
            f"Input={x/SCALE:.1f}, Output={result/SCALE:.3f}, Expectedâ‰ˆ{expected/SCALE:.3f}, Error={error:.3f}"
        )
    
    # Real sigmoid comparison (for reference)
    print(f"\nğŸ“ˆ SIGMOID ACCURACY COMPARISON:")
    for x_val in [0, 1_000_000, 2_000_000, 3_000_000, 6_000_000]:
        approx = sigmoid_approx(x_val) / SCALE
        real = 1 / (1 + math.exp(-(x_val/SCALE)))
        error = abs(approx - real)
        print(f"  x={x_val/SCALE:.1f}: Approx={approx:.3f}, Real={real:.3f}, Error={error:.3f}")


def test_edge_cases(results: TestResults):
    """Test edge cases and boundary conditions"""
    
    print("\n" + "â”€" * 70)
    print("TEST CATEGORY: Edge Cases")
    print("â”€" * 70)
    
    # Test 1: Zero weights
    data = {
        'payload': 1_000_000,
        'quality_score': 800_000,
        'category': 1
    }
    
    model_zero = {
        'weights': [0, 0, 0, 0],
        'bias': 500_000,
        'threshold': 1_000_000,
        'algorithm_id': 1
    }
    
    result = divine_future_linear(data, model_zero)
    
    results.add(
        "Edge: Zero weights (bias only)",
        result['score'] == 500_000,
        f"Score={result['score']/SCALE:.2f} (should equal bias)"
    )
    
    # Test 2: Maximum values
    data_max = {
        'payload': 10_000_000,  # $10
        'quality_score': 1_000_000,  # 1.0 (perfect quality)
        'category': 1
    }
    
    model_max = {
        'weights': [100_000, 100_000, 100_000, 100_000],
        'bias': 0,
        'threshold': 1_000_000,
        'algorithm_id': 1
    }
    
    result_max = divine_future_linear(data_max, model_max)
    
    results.add(
        "Edge: Maximum values",
        result_max['score'] > 0,
        f"Score={result_max['score']/SCALE:.2f}"
    )
    
    # Test 3: Exact threshold in tree
    data_exact = {
        'payload': 1_500_000,  # Exactly at threshold
        'quality_score': 900_000,  # Exactly at threshold
        'category': 1
    }
    
    model_tree = {
        'weights': [1_500_000, 900_000, 700_000, 0],
        'bias': 0,
        'threshold': 1_000_000,
        'algorithm_id': 3
    }
    
    result_tree = divine_future_tree(data_exact, model_tree)
    
    # When equal, Leo uses > (not >=), so this goes RIGHT branch
    results.add(
        "Edge: Exact threshold in decision tree",
        result_tree['direction'] in [True, False],  # Valid boolean
        f"Direction={result_tree['direction']}, Path={result_tree['path']}"
    )


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MAIN TEST RUNNER
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def main():
    """Run all tests"""
    
    print("â•" * 70)
    print("PROPHETIA ALGORITHM TEST SUITE - WEEK 4")
    print("â•" * 70)
    print("Testing three ML algorithms:")
    print("  1. Linear Regression (algorithm_id = 1)")
    print("  2. Logistic Regression (algorithm_id = 2)")
    print("  3. Decision Tree (algorithm_id = 3)")
    print("â•" * 70)
    
    results = TestResults()
    
    # Run all test categories
    test_linear_regression(results)
    test_logistic_regression(results)
    test_decision_tree_all_paths(results)
    test_algorithm_comparison(results)
    test_sigmoid_approximation(results)
    test_edge_cases(results)
    
    # Print summary
    results.summary()
    
    print("\n" + "â•" * 70)
    
    return results.failed == 0


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
